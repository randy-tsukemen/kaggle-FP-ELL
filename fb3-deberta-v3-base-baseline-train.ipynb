{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this notebook\n- Deberta-v3-base starter code\n- pip wheels is [here](https://www.kaggle.com/code/yasufuminakama/fb3-pip-wheels)\n- Inference notebook is [here](https://www.kaggle.com/yasufuminakama/fb3-deberta-v3-base-baseline-inference)\n\nIf this notebook is helpful, feel free to upvote :)","metadata":{"papermill":{"duration":0.012455,"end_time":"2022-08-31T07:01:57.321119","exception":false,"start_time":"2022-08-31T07:01:57.308664","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Directory settings","metadata":{"papermill":{"duration":0.011359,"end_time":"2022-08-31T07:01:57.342988","exception":false,"start_time":"2022-08-31T07:01:57.331629","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"papermill":{"duration":0.025315,"end_time":"2022-08-31T07:01:57.378897","exception":false,"start_time":"2022-08-31T07:01:57.353582","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-30T03:29:34.511308Z","iopub.execute_input":"2022-10-30T03:29:34.511638Z","iopub.status.idle":"2022-10-30T03:29:34.538196Z","shell.execute_reply.started":"2022-10-30T03:29:34.511565Z","shell.execute_reply":"2022-10-30T03:29:34.537375Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# CFG","metadata":{"papermill":{"duration":0.006569,"end_time":"2022-08-31T07:01:57.395826","exception":false,"start_time":"2022-08-31T07:01:57.389257","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    wandb=True\n    competition='FB3'\n    _wandb_kernel='nakama'\n    debug=False\n    apex=True\n    print_freq=20\n    num_workers=4\n    model=\"microsoft/deberta-v3-base\"\n    gradient_checkpointing=True\n    scheduler='cosine' # ['linear', 'cosine']\n    batch_scheduler=True\n    num_cycles=0.5\n    num_warmup_steps=0\n    epochs=4\n    encoder_lr=2e-5\n    decoder_lr=2e-5\n    min_lr=1e-6\n    eps=1e-6\n    betas=(0.9, 0.999)\n    batch_size=8\n    max_len=512\n    weight_decay=0.01\n    gradient_accumulation_steps=1\n    max_grad_norm=1000\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]\n    train=True\n    \nif CFG.debug:\n    CFG.epochs = 2\n    CFG.trn_fold = [0]","metadata":{"papermill":{"duration":0.015868,"end_time":"2022-08-31T07:01:57.417077","exception":false,"start_time":"2022-08-31T07:01:57.401209","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-30T03:30:05.355856Z","iopub.execute_input":"2022-10-30T03:30:05.356816Z","iopub.status.idle":"2022-10-30T03:30:05.364997Z","shell.execute_reply.started":"2022-10-30T03:30:05.356762Z","shell.execute_reply":"2022-10-30T03:30:05.363913Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# wandb\n# ====================================================\nif CFG.wandb:\n    \n    import wandb\n\n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n        wandb.login(key=secret_value_0)\n        anony = None\n    except:\n        anony = \"must\"\n        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n\n\n    def class2dict(f):\n        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n\n    run = wandb.init(project='FB3-Public', \n                     name=CFG.model,\n                     config=class2dict(CFG),\n                     group=CFG.model,\n                     job_type=\"train\",\n                     anonymous=anony)","metadata":{"papermill":{"duration":9.02584,"end_time":"2022-08-31T07:02:06.448566","exception":false,"start_time":"2022-08-31T07:01:57.422726","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-30T03:30:29.809624Z","iopub.execute_input":"2022-10-30T03:30:29.810333Z","iopub.status.idle":"2022-10-30T03:30:37.950113Z","shell.execute_reply.started":"2022-10-30T03:30:29.810294Z","shell.execute_reply":"2022-10-30T03:30:37.948984Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mminori11\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20221030_033031-36gbep3p</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/minori11/FB3-Public/runs/36gbep3p\" target=\"_blank\">microsoft/deberta-v3-base</a></strong> to <a href=\"https://wandb.ai/minori11/FB3-Public\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Library","metadata":{"papermill":{"duration":0.008876,"end_time":"2022-08-31T07:02:06.467728","exception":false,"start_time":"2022-08-31T07:02:06.458852","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nos.system('pip install iterative-stratification==0.1.7')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nos.system('pip uninstall -y transformers')\nos.system('pip uninstall -y tokenizers')\nos.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels transformers')\nos.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels tokenizers')\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"papermill":{"duration":57.587416,"end_time":"2022-08-31T07:03:04.064247","exception":false,"start_time":"2022-08-31T07:02:06.476831","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-30T03:38:56.850341Z","iopub.execute_input":"2022-10-30T03:38:56.851001Z","iopub.status.idle":"2022-10-30T03:39:37.591201Z","shell.execute_reply.started":"2022-10-30T03:38:56.850959Z","shell.execute_reply":"2022-10-30T03:39:37.590201Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting iterative-stratification==0.1.7\n  Downloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.0.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.21.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.7.3)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.7) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.7) (3.1.0)\nInstalling collected packages: iterative-stratification\nSuccessfully installed iterative-stratification-0.1.7\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n","output_type":"stream"},{"name":"stdout","text":"Found existing installation: transformers 4.20.1\nUninstalling transformers-4.20.1:\n  Successfully uninstalled transformers-4.20.1\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n","output_type":"stream"},{"name":"stdout","text":"Found existing installation: tokenizers 0.12.1\nUninstalling tokenizers-0.12.1:\n  Successfully uninstalled tokenizers-0.12.1\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n","output_type":"stream"},{"name":"stdout","text":"Looking in links: ../input/fb3-pip-wheels\nProcessing /kaggle/input/fb3-pip-wheels/transformers-4.21.2-py3-none-any.whl\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nProcessing /kaggle/input/fb3-pip-wheels/tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\nInstalling collected packages: tokenizers, transformers\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\nallennlp 2.10.0 requires transformers<4.21,>=4.1, but you have transformers 4.21.2 which is incompatible.\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n","output_type":"stream"},{"name":"stdout","text":"Successfully installed tokenizers-0.12.1 transformers-4.21.2\nLooking in links: ../input/fb3-pip-wheels\nRequirement already satisfied: tokenizers in /opt/conda/lib/python3.7/site-packages (0.12.1)\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n","output_type":"stream"},{"name":"stdout","text":"tokenizers.__version__: 0.12.1\ntransformers.__version__: 4.21.2\nenv: TOKENIZERS_PARALLELISM=true\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Utils","metadata":{"papermill":{"duration":0.007998,"end_time":"2022-08-31T07:03:04.079768","exception":false,"start_time":"2022-08-31T07:03:04.07177","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef MCRMSE(y_trues, y_preds):\n    scores = []\n    idxes = y_trues.shape[1]\n    for i in range(idxes):\n        y_true = y_trues[:,i]\n        y_pred = y_preds[:,i]\n        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n        scores.append(score)\n    mcrmse_score = np.mean(scores)\n    return mcrmse_score, scores\n\n\ndef get_score(y_trues, y_preds):\n    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n    return mcrmse_score, scores\n\n\ndef get_logger(filename=OUTPUT_DIR+'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","metadata":{"papermill":{"duration":0.024132,"end_time":"2022-08-31T07:03:04.111108","exception":false,"start_time":"2022-08-31T07:03:04.086976","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-30T03:46:07.422550Z","iopub.execute_input":"2022-10-30T03:46:07.423387Z","iopub.status.idle":"2022-10-30T03:46:07.449436Z","shell.execute_reply.started":"2022-10-30T03:46:07.423331Z","shell.execute_reply":"2022-10-30T03:46:07.448456Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{"papermill":{"duration":0.012589,"end_time":"2022-08-31T07:03:04.13341","exception":false,"start_time":"2022-08-31T07:03:04.120821","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Data Loading\n# ====================================================\ntrain = pd.read_csv('../input/feedback-prize-english-language-learning/train.csv')\ntest = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\nsubmission = pd.read_csv('../input/feedback-prize-english-language-learning/sample_submission.csv')\n\nprint(f\"train.shape: {train.shape}\")\ndisplay(train.head())\nprint(f\"test.shape: {test.shape}\")\ndisplay(test.head())\nprint(f\"submission.shape: {submission.shape}\")\ndisplay(submission.head())","metadata":{"papermill":{"duration":0.242687,"end_time":"2022-08-31T07:03:04.383434","exception":false,"start_time":"2022-08-31T07:03:04.140747","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-30T03:46:11.064625Z","iopub.execute_input":"2022-10-30T03:46:11.065030Z","iopub.status.idle":"2022-10-30T03:46:11.311967Z","shell.execute_reply.started":"2022-10-30T03:46:11.064998Z","shell.execute_reply":"2022-10-30T03:46:11.310947Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"train.shape: (3911, 8)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"        text_id                                          full_text  cohesion  syntax  vocabulary  phraseology  grammar  conventions\n0  0016926B079C  I think that students would benefit from learn...       3.5     3.5         3.0          3.0      4.0          3.0\n1  0022683E9EA5  When a problem is a change you have to let it ...       2.5     2.5         3.0          2.0      2.0          2.5\n2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0     3.5         3.0          3.0      3.0          2.5\n3  003885A45F42  The best time in life is when you become yours...       4.5     4.5         4.5          4.5      4.0          5.0\n4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5     3.0         3.0          3.0      2.5          2.5","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>full_text</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0016926B079C</td>\n      <td>I think that students would benefit from learn...</td>\n      <td>3.5</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0022683E9EA5</td>\n      <td>When a problem is a change you have to let it ...</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00299B378633</td>\n      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003885A45F42</td>\n      <td>The best time in life is when you become yours...</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0049B1DF5CCC</td>\n      <td>Small act of kindness can impact in other peop...</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>2.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"test.shape: (3, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"        text_id                                          full_text\n0  0000C359D63E  when a person has no experience on a job their...\n1  000BAD50D026  Do you think students would benefit from being...\n2  00367BB2546B  Thomas Jefferson once states that \"it is wonde...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>full_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>when a person has no experience on a job their...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>Do you think students would benefit from being...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>Thomas Jefferson once states that \"it is wonde...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"submission.shape: (3, 7)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"        text_id  cohesion  syntax  vocabulary  phraseology  grammar  conventions\n0  0000C359D63E       3.0     3.0         3.0          3.0      3.0          3.0\n1  000BAD50D026       3.0     3.0         3.0          3.0      3.0          3.0\n2  00367BB2546B       3.0     3.0         3.0          3.0      3.0          3.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# CV split","metadata":{"papermill":{"duration":0.008293,"end_time":"2022-08-31T07:03:04.40102","exception":false,"start_time":"2022-08-31T07:03:04.392727","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# CV split\n# ====================================================\nFold = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\nfor n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_cols])):\n    train.loc[val_index, 'fold'] = int(n)\ntrain['fold'] = train['fold'].astype(int)\ndisplay(train.groupby('fold').size())","metadata":{"papermill":{"duration":0.148391,"end_time":"2022-08-31T07:03:04.558882","exception":false,"start_time":"2022-08-31T07:03:04.410491","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-30T03:46:15.483672Z","iopub.execute_input":"2022-10-30T03:46:15.484058Z","iopub.status.idle":"2022-10-30T03:46:15.629912Z","shell.execute_reply.started":"2022-10-30T03:46:15.484027Z","shell.execute_reply":"2022-10-30T03:46:15.628762Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"fold\n0    978\n1    977\n2    978\n3    978\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"if CFG.debug:\n    display(train.groupby('fold').size())\n    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n    display(train.groupby('fold').size())","metadata":{"papermill":{"duration":0.019618,"end_time":"2022-08-31T07:03:04.586414","exception":false,"start_time":"2022-08-31T07:03:04.566796","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-30T03:46:16.638117Z","iopub.execute_input":"2022-10-30T03:46:16.638467Z","iopub.status.idle":"2022-10-30T03:46:16.646795Z","shell.execute_reply.started":"2022-10-30T03:46:16.638438Z","shell.execute_reply":"2022-10-30T03:46:16.645647Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# tokenizer","metadata":{"papermill":{"duration":0.007561,"end_time":"2022-08-31T07:03:04.604916","exception":false,"start_time":"2022-08-31T07:03:04.597355","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\ntokenizer = AutoTokenizer.from_pretrained(CFG.model)\ntokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\nCFG.tokenizer = tokenizer","metadata":{"papermill":{"duration":7.351568,"end_time":"2022-08-31T07:03:11.964298","exception":false,"start_time":"2022-08-31T07:03:04.61273","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-30T03:46:17.614826Z","iopub.execute_input":"2022-10-30T03:46:17.615193Z","iopub.status.idle":"2022-10-30T03:46:21.627244Z","shell.execute_reply.started":"2022-10-30T03:46:17.615163Z","shell.execute_reply":"2022-10-30T03:46:21.625937Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c67d055318c84e2a8edef584c1f2b6c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d73cd186758f4b9ea869825ebcb1de91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spm.model:   0%|          | 0.00/2.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c8d921873134aacaa1f2f98f1ddb7a7"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Dataset","metadata":{"papermill":{"duration":0.008127,"end_time":"2022-08-31T07:03:11.985369","exception":false,"start_time":"2022-08-31T07:03:11.977242","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Define max_len\n# ====================================================\nlengths = []\ntk0 = tqdm(train['full_text'].fillna(\"\").values, total=len(train))\nfor text in tk0:\n    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n    lengths.append(length)\nCFG.max_len = max(lengths) + 3 # cls & sep & sep\nLOGGER.info(f\"max_len: {CFG.max_len}\")","metadata":{"papermill":{"duration":5.893032,"end_time":"2022-08-31T07:03:17.886504","exception":false,"start_time":"2022-08-31T07:03:11.993472","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-30T03:46:21.629231Z","iopub.execute_input":"2022-10-30T03:46:21.629581Z","iopub.status.idle":"2022-10-30T03:46:27.330002Z","shell.execute_reply.started":"2022-10-30T03:46:21.629546Z","shell.execute_reply":"2022-10-30T03:46:27.329001Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3911 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3020a06b27f45aeb1a1a5c8b373ffa6"}},"metadata":{}},{"name":"stderr","text":"max_len: 1429\n","output_type":"stream"}]},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(cfg, text):\n    inputs = cfg.tokenizer.encode_plus(\n        text, \n        return_tensors=None, \n        add_special_tokens=True, \n        max_length=CFG.max_len,\n        pad_to_max_length=True,\n        truncation=True\n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TrainDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['full_text'].values\n        self.labels = df[cfg.target_cols].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        label = torch.tensor(self.labels[item], dtype=torch.float)\n        return inputs, label\n    \n\ndef collate(inputs):\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    for k, v in inputs.items():\n        inputs[k] = inputs[k][:,:mask_len]\n    return inputs","metadata":{"papermill":{"duration":0.020447,"end_time":"2022-08-31T07:03:17.916566","exception":false,"start_time":"2022-08-31T07:03:17.896119","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-30T03:46:27.331478Z","iopub.execute_input":"2022-10-30T03:46:27.332544Z","iopub.status.idle":"2022-10-30T03:46:27.343080Z","shell.execute_reply.started":"2022-10-30T03:46:27.332504Z","shell.execute_reply":"2022-10-30T03:46:27.342334Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.008073,"end_time":"2022-08-31T07:03:17.933189","exception":false,"start_time":"2022-08-31T07:03:17.925116","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n    \n\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n            self.config.hidden_dropout = 0.\n            self.config.hidden_dropout_prob = 0.\n            self.config.attention_dropout = 0.\n            self.config.attention_probs_dropout_prob = 0.\n            LOGGER.info(self.config)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel(self.config)\n        if self.cfg.gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n        self.pool = MeanPooling()\n        self.fc = nn.Linear(self.config.hidden_size, 6)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(feature)\n        return output","metadata":{"papermill":{"duration":0.033105,"end_time":"2022-08-31T07:03:17.97447","exception":false,"start_time":"2022-08-31T07:03:17.941365","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-30T03:46:27.345966Z","iopub.execute_input":"2022-10-30T03:46:27.346643Z","iopub.status.idle":"2022-10-30T03:46:27.363424Z","shell.execute_reply.started":"2022-10-30T03:46:27.346608Z","shell.execute_reply":"2022-10-30T03:46:27.362528Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Loss","metadata":{"papermill":{"duration":0.008106,"end_time":"2022-08-31T07:03:17.993861","exception":false,"start_time":"2022-08-31T07:03:17.985755","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Loss\n# ====================================================\nclass RMSELoss(nn.Module):\n    def __init__(self, reduction='mean', eps=1e-9):\n        super().__init__()\n        self.mse = nn.MSELoss(reduction='none')\n        self.reduction = reduction\n        self.eps = eps\n\n    def forward(self, y_pred, y_true):\n        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n        if self.reduction == 'none':\n            loss = loss\n        elif self.reduction == 'sum':\n            loss = loss.sum()\n        elif self.reduction == 'mean':\n            loss = loss.mean()\n        return loss","metadata":{"papermill":{"duration":0.021697,"end_time":"2022-08-31T07:03:18.02376","exception":false,"start_time":"2022-08-31T07:03:18.002063","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-30T03:46:27.364683Z","iopub.execute_input":"2022-10-30T03:46:27.365204Z","iopub.status.idle":"2022-10-30T03:46:27.382420Z","shell.execute_reply.started":"2022-10-30T03:46:27.365168Z","shell.execute_reply":"2022-10-30T03:46:27.380881Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Helpler functions","metadata":{"papermill":{"duration":0.008452,"end_time":"2022-08-31T07:03:18.041557","exception":false,"start_time":"2022-08-31T07:03:18.033105","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Helper functions\n# ====================================================\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n\ndef train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    model.train()\n    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n    losses = AverageMeter()\n    start = end = time.time()\n    global_step = 0\n    for step, (inputs, labels) in enumerate(train_loader):\n        inputs = collate(inputs)\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        with torch.cuda.amp.autocast(enabled=CFG.apex):\n            y_preds = model(inputs)\n            loss = criterion(y_preds, labels)\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        scaler.scale(loss).backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            global_step += 1\n            if CFG.batch_scheduler:\n                scheduler.step()\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Grad: {grad_norm:.4f}  '\n                  'LR: {lr:.8f}  '\n                  .format(epoch+1, step, len(train_loader), \n                          remain=timeSince(start, float(step+1)/len(train_loader)),\n                          loss=losses,\n                          grad_norm=grad_norm,\n                          lr=scheduler.get_lr()[0]))\n        if CFG.wandb:\n            wandb.log({f\"[fold{fold}] loss\": losses.val,\n                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n    return losses.avg\n\n\ndef valid_fn(valid_loader, model, criterion, device):\n    losses = AverageMeter()\n    model.eval()\n    preds = []\n    start = end = time.time()\n    for step, (inputs, labels) in enumerate(valid_loader):\n        inputs = collate(inputs)\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        with torch.no_grad():\n            y_preds = model(inputs)\n            loss = criterion(y_preds, labels)\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        preds.append(y_preds.to('cpu').numpy())\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n            print('EVAL: [{0}/{1}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(step, len(valid_loader),\n                          loss=losses,\n                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n    predictions = np.concatenate(preds)\n    return losses.avg, predictions","metadata":{"papermill":{"duration":0.030759,"end_time":"2022-08-31T07:03:18.08056","exception":false,"start_time":"2022-08-31T07:03:18.049801","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-30T03:46:27.387647Z","iopub.execute_input":"2022-10-30T03:46:27.389276Z","iopub.status.idle":"2022-10-30T03:46:27.420169Z","shell.execute_reply.started":"2022-10-30T03:46:27.389239Z","shell.execute_reply":"2022-10-30T03:46:27.418587Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# train loop","metadata":{"papermill":{"duration":0.0081,"end_time":"2022-08-31T07:03:18.100232","exception":false,"start_time":"2022-08-31T07:03:18.092132","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# train loop\n# ====================================================\ndef train_loop(folds, fold):\n    \n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n\n    # ====================================================\n    # loader\n    # ====================================================\n    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n    valid_labels = valid_folds[CFG.target_cols].values\n    \n    train_dataset = TrainDataset(CFG, train_folds)\n    valid_dataset = TrainDataset(CFG, valid_folds)\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=True,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=CFG.batch_size * 2,\n                              shuffle=False,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n    model = CustomModel(CFG, config_path=None, pretrained=True)\n    torch.save(model.config, OUTPUT_DIR+'config.pth')\n    model.to(device)\n    \n    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': weight_decay},\n            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': 0.0},\n            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n             'lr': decoder_lr, 'weight_decay': 0.0}\n        ]\n        return optimizer_parameters\n\n    optimizer_parameters = get_optimizer_params(model,\n                                                encoder_lr=CFG.encoder_lr, \n                                                decoder_lr=CFG.decoder_lr,\n                                                weight_decay=CFG.weight_decay)\n    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n    \n    # ====================================================\n    # scheduler\n    # ====================================================\n    def get_scheduler(cfg, optimizer, num_train_steps):\n        if cfg.scheduler == 'linear':\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n            )\n        elif cfg.scheduler == 'cosine':\n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n            )\n        return scheduler\n    \n    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n\n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = nn.SmoothL1Loss(reduction='mean') # RMSELoss(reduction=\"mean\")\n    \n    best_score = np.inf\n\n    for epoch in range(CFG.epochs):\n\n        start_time = time.time()\n\n        # train\n        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n\n        # eval\n        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n        \n        # scoring\n        score, scores = get_score(valid_labels, predictions)\n\n        elapsed = time.time() - start_time\n\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n        if CFG.wandb:\n            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n                       f\"[fold{fold}] score\": score})\n        \n        if best_score > score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save({'model': model.state_dict(),\n                        'predictions': predictions},\n                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n\n    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n                             map_location=torch.device('cpu'))['predictions']\n    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return valid_folds","metadata":{"papermill":{"duration":0.033332,"end_time":"2022-08-31T07:03:18.141812","exception":false,"start_time":"2022-08-31T07:03:18.10848","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-30T03:46:27.425331Z","iopub.execute_input":"2022-10-30T03:46:27.428222Z","iopub.status.idle":"2022-10-30T03:46:27.447880Z","shell.execute_reply.started":"2022-10-30T03:46:27.428193Z","shell.execute_reply":"2022-10-30T03:46:27.446755Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    \n    def get_result(oof_df):\n        labels = oof_df[CFG.target_cols].values\n        preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n        score, scores = get_score(labels, preds)\n        LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n    \n    if CFG.train:\n        oof_df = pd.DataFrame()\n        for fold in range(CFG.n_fold):\n            if fold in CFG.trn_fold:\n                _oof_df = train_loop(train, fold)\n                oof_df = pd.concat([oof_df, _oof_df])\n                LOGGER.info(f\"========== fold: {fold} result ==========\")\n                get_result(_oof_df)\n        oof_df = oof_df.reset_index(drop=True)\n        LOGGER.info(f\"========== CV ==========\")\n        get_result(oof_df)\n        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n        \n    if CFG.wandb:\n        wandb.finish()","metadata":{"papermill":{"duration":11935.46951,"end_time":"2022-08-31T10:22:13.621316","exception":false,"start_time":"2022-08-31T07:03:18.151806","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-30T03:46:27.453376Z","iopub.execute_input":"2022-10-30T03:46:27.456181Z","iopub.status.idle":"2022-10-30T07:04:45.848935Z","shell.execute_reply.started":"2022-10-30T03:46:27.456142Z","shell.execute_reply":"2022-10-30T07:04:45.848001Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"========== fold: 0 training ==========\nDebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_dropout\": 0.0,\n  \"attention_probs_dropout_prob\": 0.0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout\": 0.0,\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.21.2\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/354M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcf7b6ac8a5144eba587bcecace1dd30"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias']\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Epoch: [1][0/366] Elapsed 0m 2s (remain 17m 53s) Loss: 2.3540(2.3540) Grad: inf  LR: 0.00002000  \nEpoch: [1][20/366] Elapsed 0m 39s (remain 10m 44s) Loss: 0.3456(1.3301) Grad: 183186.4844  LR: 0.00001999  \nEpoch: [1][40/366] Elapsed 1m 15s (remain 9m 59s) Loss: 0.1139(0.7888) Grad: 59198.6914  LR: 0.00001996  \nEpoch: [1][60/366] Elapsed 1m 48s (remain 9m 3s) Loss: 0.1754(0.5886) Grad: 165556.0469  LR: 0.00001991  \nEpoch: [1][80/366] Elapsed 2m 25s (remain 8m 32s) Loss: 0.1429(0.4814) Grad: 84974.4297  LR: 0.00001985  \nEpoch: [1][100/366] Elapsed 3m 8s (remain 8m 14s) Loss: 0.0978(0.4215) Grad: 63966.6523  LR: 0.00001977  \nEpoch: [1][120/366] Elapsed 3m 44s (remain 7m 34s) Loss: 0.1825(0.3740) Grad: 93032.9844  LR: 0.00001967  \nEpoch: [1][140/366] Elapsed 4m 22s (remain 6m 59s) Loss: 0.1633(0.3375) Grad: 84404.7266  LR: 0.00001955  \nEpoch: [1][160/366] Elapsed 4m 58s (remain 6m 20s) Loss: 0.1191(0.3099) Grad: 76614.7109  LR: 0.00001941  \nEpoch: [1][180/366] Elapsed 5m 35s (remain 5m 42s) Loss: 0.1685(0.2896) Grad: 68048.8359  LR: 0.00001926  \nEpoch: [1][200/366] Elapsed 6m 15s (remain 5m 7s) Loss: 0.1487(0.2738) Grad: 80228.6953  LR: 0.00001909  \nEpoch: [1][220/366] Elapsed 6m 48s (remain 4m 28s) Loss: 0.1824(0.2617) Grad: 135106.2656  LR: 0.00001890  \nEpoch: [1][240/366] Elapsed 7m 26s (remain 3m 51s) Loss: 0.1147(0.2499) Grad: 42691.2656  LR: 0.00001870  \nEpoch: [1][260/366] Elapsed 8m 3s (remain 3m 14s) Loss: 0.0801(0.2391) Grad: 33266.6758  LR: 0.00001848  \nEpoch: [1][280/366] Elapsed 8m 35s (remain 2m 35s) Loss: 0.1273(0.2295) Grad: 66755.5312  LR: 0.00001824  \nEpoch: [1][300/366] Elapsed 9m 8s (remain 1m 58s) Loss: 0.1077(0.2219) Grad: 48647.6562  LR: 0.00001799  \nEpoch: [1][320/366] Elapsed 9m 46s (remain 1m 22s) Loss: 0.1017(0.2154) Grad: 78427.7344  LR: 0.00001773  \nEpoch: [1][340/366] Elapsed 10m 21s (remain 0m 45s) Loss: 0.1386(0.2095) Grad: 98316.7266  LR: 0.00001745  \nEpoch: [1][360/366] Elapsed 10m 53s (remain 0m 9s) Loss: 0.1250(0.2040) Grad: 44562.1406  LR: 0.00001715  \nEpoch: [1][365/366] Elapsed 11m 1s (remain 0m 0s) Loss: 0.1111(0.2027) Grad: 48393.4727  LR: 0.00001708  \nEVAL: [0/62] Elapsed 0m 1s (remain 1m 12s) Loss: 0.0761(0.0761) \nEVAL: [20/62] Elapsed 0m 25s (remain 0m 50s) Loss: 0.0805(0.1048) \nEVAL: [40/62] Elapsed 0m 53s (remain 0m 27s) Loss: 0.0912(0.1042) \n","output_type":"stream"},{"name":"stderr","text":"Epoch 1 - avg_train_loss: 0.2027  avg_val_loss: 0.1061  time: 738s\nEpoch 1 - Score: 0.4611  Scores: [0.4993728326401723, 0.45735299106394384, 0.4183196516648491, 0.46281486367050245, 0.4764436580744625, 0.4520492500313036]\nEpoch 1 - Save Best Score: 0.4611 Model\n","output_type":"stream"},{"name":"stdout","text":"EVAL: [60/62] Elapsed 1m 16s (remain 0m 1s) Loss: 0.1000(0.1061) \nEVAL: [61/62] Elapsed 1m 16s (remain 0m 0s) Loss: 0.1205(0.1061) \nEpoch: [2][0/366] Elapsed 0m 1s (remain 7m 42s) Loss: 0.1073(0.1073) Grad: 168138.8906  LR: 0.00001706  \nEpoch: [2][20/366] Elapsed 0m 30s (remain 8m 19s) Loss: 0.1586(0.1029) Grad: 209359.5625  LR: 0.00001675  \nEpoch: [2][40/366] Elapsed 1m 8s (remain 9m 2s) Loss: 0.0897(0.1053) Grad: 130562.8438  LR: 0.00001643  \nEpoch: [2][60/366] Elapsed 1m 51s (remain 9m 17s) Loss: 0.1193(0.1041) Grad: 96434.8672  LR: 0.00001610  \nEpoch: [2][80/366] Elapsed 2m 29s (remain 8m 46s) Loss: 0.0757(0.1024) Grad: 64706.9141  LR: 0.00001575  \nEpoch: [2][100/366] Elapsed 3m 4s (remain 8m 4s) Loss: 0.1591(0.1028) Grad: 204630.4375  LR: 0.00001540  \nEpoch: [2][120/366] Elapsed 3m 41s (remain 7m 29s) Loss: 0.1117(0.1031) Grad: 84246.7188  LR: 0.00001503  \nEpoch: [2][140/366] Elapsed 4m 16s (remain 6m 49s) Loss: 0.0818(0.1024) Grad: 35237.5156  LR: 0.00001466  \nEpoch: [2][160/366] Elapsed 4m 54s (remain 6m 14s) Loss: 0.1322(0.1029) Grad: 109012.7500  LR: 0.00001427  \nEpoch: [2][180/366] Elapsed 5m 29s (remain 5m 36s) Loss: 0.1022(0.1016) Grad: 65746.1094  LR: 0.00001388  \nEpoch: [2][200/366] Elapsed 6m 10s (remain 5m 3s) Loss: 0.1154(0.1015) Grad: 73481.8906  LR: 0.00001348  \nEpoch: [2][220/366] Elapsed 6m 47s (remain 4m 27s) Loss: 0.1032(0.1007) Grad: 93031.0703  LR: 0.00001308  \nEpoch: [2][240/366] Elapsed 7m 20s (remain 3m 48s) Loss: 0.1037(0.1008) Grad: 64340.3281  LR: 0.00001267  \nEpoch: [2][260/366] Elapsed 7m 51s (remain 3m 9s) Loss: 0.1266(0.1017) Grad: 82829.8750  LR: 0.00001225  \nEpoch: [2][280/366] Elapsed 8m 21s (remain 2m 31s) Loss: 0.1411(0.1011) Grad: 137408.9844  LR: 0.00001183  \nEpoch: [2][300/366] Elapsed 8m 57s (remain 1m 56s) Loss: 0.0933(0.1015) Grad: 113396.8672  LR: 0.00001141  \nEpoch: [2][320/366] Elapsed 9m 30s (remain 1m 20s) Loss: 0.1488(0.1013) Grad: 119291.8906  LR: 0.00001098  \nEpoch: [2][340/366] Elapsed 10m 11s (remain 0m 44s) Loss: 0.1454(0.1012) Grad: 187571.1094  LR: 0.00001056  \nEpoch: [2][360/366] Elapsed 10m 46s (remain 0m 8s) Loss: 0.1022(0.1013) Grad: 38232.5117  LR: 0.00001013  \nEpoch: [2][365/366] Elapsed 10m 53s (remain 0m 0s) Loss: 0.1370(0.1014) Grad: 61747.6445  LR: 0.00001002  \nEVAL: [0/62] Elapsed 0m 1s (remain 1m 12s) Loss: 0.0740(0.0740) \nEVAL: [20/62] Elapsed 0m 25s (remain 0m 49s) Loss: 0.0788(0.1015) \nEVAL: [40/62] Elapsed 0m 52s (remain 0m 27s) Loss: 0.0881(0.1029) \n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 - avg_train_loss: 0.1014  avg_val_loss: 0.1044  time: 729s\nEpoch 2 - Score: 0.4572  Scores: [0.5009224150751537, 0.449344901658793, 0.41075302181363027, 0.4595055386759559, 0.4683918339433486, 0.4545239496670008]\nEpoch 2 - Save Best Score: 0.4572 Model\n","output_type":"stream"},{"name":"stdout","text":"EVAL: [60/62] Elapsed 1m 15s (remain 0m 1s) Loss: 0.0942(0.1044) \nEVAL: [61/62] Elapsed 1m 15s (remain 0m 0s) Loss: 0.1204(0.1044) \nEpoch: [3][0/366] Elapsed 0m 1s (remain 9m 51s) Loss: 0.1092(0.1092) Grad: 276885.5000  LR: 0.00001000  \nEpoch: [3][20/366] Elapsed 0m 32s (remain 8m 51s) Loss: 0.0735(0.0943) Grad: 90004.5625  LR: 0.00000957  \nEpoch: [3][40/366] Elapsed 1m 8s (remain 9m 3s) Loss: 0.0701(0.0940) Grad: 57381.9062  LR: 0.00000914  \nEpoch: [3][60/366] Elapsed 1m 49s (remain 9m 5s) Loss: 0.0583(0.0925) Grad: 99001.2656  LR: 0.00000872  \nEpoch: [3][80/366] Elapsed 2m 22s (remain 8m 22s) Loss: 0.0795(0.0922) Grad: 74363.3984  LR: 0.00000829  \nEpoch: [3][100/366] Elapsed 3m 1s (remain 7m 55s) Loss: 0.0667(0.0929) Grad: 98360.3906  LR: 0.00000787  \nEpoch: [3][120/366] Elapsed 3m 39s (remain 7m 24s) Loss: 0.1012(0.0939) Grad: 66590.2891  LR: 0.00000746  \nEpoch: [3][140/366] Elapsed 4m 13s (remain 6m 44s) Loss: 0.0595(0.0945) Grad: 110581.5625  LR: 0.00000704  \nEpoch: [3][160/366] Elapsed 4m 48s (remain 6m 7s) Loss: 0.0928(0.0944) Grad: 124052.4844  LR: 0.00000664  \nEpoch: [3][180/366] Elapsed 5m 21s (remain 5m 28s) Loss: 0.1435(0.0945) Grad: 68823.1641  LR: 0.00000624  \nEpoch: [3][200/366] Elapsed 6m 9s (remain 5m 3s) Loss: 0.1109(0.0944) Grad: 106205.7891  LR: 0.00000584  \nEpoch: [3][220/366] Elapsed 6m 44s (remain 4m 25s) Loss: 0.0929(0.0942) Grad: 85074.1172  LR: 0.00000546  \nEpoch: [3][240/366] Elapsed 7m 23s (remain 3m 49s) Loss: 0.0997(0.0935) Grad: 119982.2500  LR: 0.00000508  \nEpoch: [3][260/366] Elapsed 8m 5s (remain 3m 15s) Loss: 0.0916(0.0935) Grad: 88820.8750  LR: 0.00000471  \nEpoch: [3][280/366] Elapsed 8m 38s (remain 2m 36s) Loss: 0.0879(0.0930) Grad: 65249.2344  LR: 0.00000435  \nEpoch: [3][300/366] Elapsed 9m 11s (remain 1m 59s) Loss: 0.0872(0.0932) Grad: 125846.5703  LR: 0.00000400  \nEpoch: [3][320/366] Elapsed 9m 42s (remain 1m 21s) Loss: 0.1365(0.0937) Grad: 138649.8906  LR: 0.00000367  \nEpoch: [3][340/366] Elapsed 10m 19s (remain 0m 45s) Loss: 0.1169(0.0941) Grad: 65304.0078  LR: 0.00000334  \nEpoch: [3][360/366] Elapsed 10m 48s (remain 0m 8s) Loss: 0.0572(0.0942) Grad: 48624.6836  LR: 0.00000303  \nEpoch: [3][365/366] Elapsed 10m 56s (remain 0m 0s) Loss: 0.0844(0.0942) Grad: 81895.7031  LR: 0.00000295  \nEVAL: [0/62] Elapsed 0m 1s (remain 1m 13s) Loss: 0.0597(0.0597) \nEVAL: [20/62] Elapsed 0m 25s (remain 0m 50s) Loss: 0.0779(0.1001) \nEVAL: [40/62] Elapsed 0m 52s (remain 0m 27s) Loss: 0.0879(0.1017) \n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 - avg_train_loss: 0.0942  avg_val_loss: 0.1028  time: 732s\nEpoch 3 - Score: 0.4538  Scores: [0.49292357213205973, 0.44496869086724306, 0.4122862033965971, 0.4600363325064451, 0.4682046793635656, 0.4441536069293712]\nEpoch 3 - Save Best Score: 0.4538 Model\n","output_type":"stream"},{"name":"stdout","text":"EVAL: [60/62] Elapsed 1m 15s (remain 0m 1s) Loss: 0.0868(0.1028) \nEVAL: [61/62] Elapsed 1m 15s (remain 0m 0s) Loss: 0.1056(0.1028) \nEpoch: [4][0/366] Elapsed 0m 2s (remain 15m 48s) Loss: 0.1197(0.1197) Grad: 385110.3125  LR: 0.00000294  \nEpoch: [4][20/366] Elapsed 0m 37s (remain 10m 17s) Loss: 0.0621(0.0882) Grad: 161985.4844  LR: 0.00000264  \nEpoch: [4][40/366] Elapsed 1m 11s (remain 9m 27s) Loss: 0.0894(0.0898) Grad: 210287.2656  LR: 0.00000236  \nEpoch: [4][60/366] Elapsed 1m 45s (remain 8m 49s) Loss: 0.0821(0.0874) Grad: 190272.7344  LR: 0.00000209  \nEpoch: [4][80/366] Elapsed 2m 22s (remain 8m 21s) Loss: 0.1449(0.0889) Grad: 193887.0312  LR: 0.00000183  \nEpoch: [4][100/366] Elapsed 3m 0s (remain 7m 53s) Loss: 0.1184(0.0909) Grad: 152079.2500  LR: 0.00000159  \nEpoch: [4][120/366] Elapsed 3m 36s (remain 7m 18s) Loss: 0.1034(0.0911) Grad: 244802.5000  LR: 0.00000137  \nEpoch: [4][140/366] Elapsed 4m 7s (remain 6m 35s) Loss: 0.0802(0.0898) Grad: 119890.3125  LR: 0.00000116  \nEpoch: [4][160/366] Elapsed 4m 46s (remain 6m 4s) Loss: 0.1396(0.0905) Grad: 204615.2188  LR: 0.00000097  \nEpoch: [4][180/366] Elapsed 5m 22s (remain 5m 29s) Loss: 0.1006(0.0892) Grad: 57064.6719  LR: 0.00000079  \nEpoch: [4][200/366] Elapsed 5m 59s (remain 4m 55s) Loss: 0.0586(0.0893) Grad: 62779.9375  LR: 0.00000063  \nEpoch: [4][220/366] Elapsed 6m 28s (remain 4m 14s) Loss: 0.0566(0.0891) Grad: 94241.0625  LR: 0.00000049  \nEpoch: [4][240/366] Elapsed 7m 8s (remain 3m 42s) Loss: 0.0753(0.0889) Grad: 65295.9141  LR: 0.00000037  \nEpoch: [4][260/366] Elapsed 7m 44s (remain 3m 6s) Loss: 0.1019(0.0892) Grad: 131370.3906  LR: 0.00000026  \nEpoch: [4][280/366] Elapsed 8m 19s (remain 2m 31s) Loss: 0.0617(0.0888) Grad: 74462.7656  LR: 0.00000017  \nEpoch: [4][300/366] Elapsed 8m 56s (remain 1m 55s) Loss: 0.1236(0.0883) Grad: 175221.0312  LR: 0.00000010  \nEpoch: [4][320/366] Elapsed 9m 30s (remain 1m 19s) Loss: 0.0658(0.0880) Grad: 70962.2891  LR: 0.00000005  \nEpoch: [4][340/366] Elapsed 10m 6s (remain 0m 44s) Loss: 0.0947(0.0882) Grad: 116638.7109  LR: 0.00000002  \nEpoch: [4][360/366] Elapsed 10m 45s (remain 0m 8s) Loss: 0.0854(0.0882) Grad: 66257.5156  LR: 0.00000000  \nEpoch: [4][365/366] Elapsed 10m 53s (remain 0m 0s) Loss: 0.0871(0.0883) Grad: 41916.6914  LR: 0.00000000  \nEVAL: [0/62] Elapsed 0m 1s (remain 1m 11s) Loss: 0.0620(0.0620) \nEVAL: [20/62] Elapsed 0m 25s (remain 0m 49s) Loss: 0.0770(0.0991) \nEVAL: [40/62] Elapsed 0m 52s (remain 0m 27s) Loss: 0.0844(0.1008) \n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 - avg_train_loss: 0.0883  avg_val_loss: 0.1018  time: 730s\nEpoch 4 - Score: 0.4514  Scores: [0.4895928186538406, 0.44267970752991154, 0.40984190621392225, 0.4585880292648106, 0.4662531891866598, 0.4417411263105253]\nEpoch 4 - Save Best Score: 0.4514 Model\n","output_type":"stream"},{"name":"stdout","text":"EVAL: [60/62] Elapsed 1m 15s (remain 0m 1s) Loss: 0.0888(0.1018) \nEVAL: [61/62] Elapsed 1m 15s (remain 0m 0s) Loss: 0.0975(0.1018) \n","output_type":"stream"},{"name":"stderr","text":"========== fold: 0 result ==========\nScore: 0.4514  Scores: [0.4895928186538406, 0.44267970752991154, 0.40984190621392225, 0.4585880292648106, 0.4662531891866598, 0.4417411263105253]\n========== fold: 1 training ==========\nDebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_dropout\": 0.0,\n  \"attention_probs_dropout_prob\": 0.0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout\": 0.0,\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.21.2\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nSome weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias']\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Epoch: [1][0/366] Elapsed 0m 1s (remain 8m 55s) Loss: 2.4673(2.4673) Grad: inf  LR: 0.00002000  \nEpoch: [1][20/366] Elapsed 0m 40s (remain 11m 6s) Loss: 0.5607(1.5055) Grad: 161435.1094  LR: 0.00001999  \nEpoch: [1][40/366] Elapsed 1m 19s (remain 10m 27s) Loss: 0.2577(0.8751) Grad: 79327.0312  LR: 0.00001996  \nEpoch: [1][60/366] Elapsed 1m 58s (remain 9m 53s) Loss: 0.1089(0.6394) Grad: 60311.0234  LR: 0.00001991  \nEpoch: [1][80/366] Elapsed 2m 37s (remain 9m 13s) Loss: 0.1744(0.5147) Grad: 118573.3906  LR: 0.00001985  \nEpoch: [1][100/366] Elapsed 3m 16s (remain 8m 35s) Loss: 0.1958(0.4436) Grad: 55061.1250  LR: 0.00001977  \nEpoch: [1][120/366] Elapsed 3m 52s (remain 7m 50s) Loss: 0.0811(0.3912) Grad: 35935.8633  LR: 0.00001967  \nEpoch: [1][140/366] Elapsed 4m 26s (remain 7m 5s) Loss: 0.2156(0.3552) Grad: 86383.4062  LR: 0.00001955  \nEpoch: [1][160/366] Elapsed 5m 1s (remain 6m 24s) Loss: 0.0831(0.3243) Grad: 55587.4922  LR: 0.00001941  \nEpoch: [1][180/366] Elapsed 5m 35s (remain 5m 42s) Loss: 0.0824(0.3025) Grad: 34009.1992  LR: 0.00001926  \nEpoch: [1][200/366] Elapsed 6m 7s (remain 5m 2s) Loss: 0.1408(0.2835) Grad: 52641.3789  LR: 0.00001909  \nEpoch: [1][220/366] Elapsed 6m 41s (remain 4m 23s) Loss: 0.0873(0.2685) Grad: 44115.1875  LR: 0.00001890  \nEpoch: [1][240/366] Elapsed 7m 18s (remain 3m 47s) Loss: 0.1377(0.2556) Grad: 36041.0117  LR: 0.00001870  \nEpoch: [1][260/366] Elapsed 7m 55s (remain 3m 11s) Loss: 0.1080(0.2446) Grad: 59115.5273  LR: 0.00001848  \nEpoch: [1][280/366] Elapsed 8m 28s (remain 2m 33s) Loss: 0.1379(0.2359) Grad: 83049.4844  LR: 0.00001824  \nEpoch: [1][300/366] Elapsed 9m 4s (remain 1m 57s) Loss: 0.1551(0.2280) Grad: 46721.8594  LR: 0.00001799  \nEpoch: [1][320/366] Elapsed 9m 39s (remain 1m 21s) Loss: 0.1063(0.2208) Grad: 88305.8594  LR: 0.00001773  \nEpoch: [1][340/366] Elapsed 10m 15s (remain 0m 45s) Loss: 0.0973(0.2147) Grad: 47043.4844  LR: 0.00001745  \nEpoch: [1][360/366] Elapsed 10m 48s (remain 0m 8s) Loss: 0.1415(0.2094) Grad: 55639.5547  LR: 0.00001716  \nEpoch: [1][365/366] Elapsed 10m 54s (remain 0m 0s) Loss: 0.0953(0.2080) Grad: 26686.5293  LR: 0.00001708  \nEVAL: [0/62] Elapsed 0m 1s (remain 1m 6s) Loss: 0.1087(0.1087) \nEVAL: [20/62] Elapsed 0m 26s (remain 0m 50s) Loss: 0.1119(0.1248) \nEVAL: [40/62] Elapsed 0m 49s (remain 0m 25s) Loss: 0.1171(0.1251) \n","output_type":"stream"},{"name":"stderr","text":"Epoch 1 - avg_train_loss: 0.2080  avg_val_loss: 0.1257  time: 732s\nEpoch 1 - Score: 0.5028  Scores: [0.5395894362027702, 0.4983513184801715, 0.42978653503696884, 0.5150542152701406, 0.5348390487684229, 0.4993887671724777]\nEpoch 1 - Save Best Score: 0.5028 Model\n","output_type":"stream"},{"name":"stdout","text":"EVAL: [60/62] Elapsed 1m 16s (remain 0m 1s) Loss: 0.1197(0.1258) \nEVAL: [61/62] Elapsed 1m 16s (remain 0m 0s) Loss: 0.0979(0.1257) \nEpoch: [2][0/366] Elapsed 0m 1s (remain 7m 16s) Loss: 0.1023(0.1023) Grad: 225693.4688  LR: 0.00001707  \nEpoch: [2][20/366] Elapsed 0m 34s (remain 9m 27s) Loss: 0.0924(0.1083) Grad: 200527.4688  LR: 0.00001676  \nEpoch: [2][40/366] Elapsed 1m 9s (remain 9m 8s) Loss: 0.0675(0.1075) Grad: 131402.7656  LR: 0.00001644  \nEpoch: [2][60/366] Elapsed 1m 42s (remain 8m 30s) Loss: 0.0881(0.1051) Grad: 87960.7188  LR: 0.00001610  \nEpoch: [2][80/366] Elapsed 2m 12s (remain 7m 44s) Loss: 0.0809(0.1028) Grad: 98913.0781  LR: 0.00001576  \nEpoch: [2][100/366] Elapsed 2m 48s (remain 7m 22s) Loss: 0.1346(0.1028) Grad: 249320.4688  LR: 0.00001540  \nEpoch: [2][120/366] Elapsed 3m 26s (remain 6m 58s) Loss: 0.0849(0.1015) Grad: 252273.9375  LR: 0.00001504  \nEpoch: [2][140/366] Elapsed 4m 4s (remain 6m 29s) Loss: 0.0964(0.1013) Grad: 71873.2031  LR: 0.00001466  \nEpoch: [2][160/366] Elapsed 4m 43s (remain 6m 0s) Loss: 0.0950(0.1020) Grad: 81084.2969  LR: 0.00001428  \nEpoch: [2][180/366] Elapsed 5m 17s (remain 5m 24s) Loss: 0.1102(0.1018) Grad: 57426.8477  LR: 0.00001389  \nEpoch: [2][200/366] Elapsed 5m 48s (remain 4m 45s) Loss: 0.0661(0.1009) Grad: 96591.7188  LR: 0.00001349  \nEpoch: [2][220/366] Elapsed 6m 28s (remain 4m 14s) Loss: 0.0898(0.0998) Grad: 101744.5703  LR: 0.00001309  \nEpoch: [2][240/366] Elapsed 7m 6s (remain 3m 41s) Loss: 0.0998(0.0993) Grad: 83712.7422  LR: 0.00001268  \nEpoch: [2][260/366] Elapsed 7m 41s (remain 3m 5s) Loss: 0.0929(0.0991) Grad: 77230.6328  LR: 0.00001226  \nEpoch: [2][280/366] Elapsed 8m 15s (remain 2m 29s) Loss: 0.0676(0.0993) Grad: 76246.0469  LR: 0.00001184  \nEpoch: [2][300/366] Elapsed 8m 54s (remain 1m 55s) Loss: 0.0657(0.0993) Grad: 61760.8750  LR: 0.00001142  \nEpoch: [2][320/366] Elapsed 9m 31s (remain 1m 20s) Loss: 0.1381(0.0990) Grad: 131941.2812  LR: 0.00001099  \nEpoch: [2][340/366] Elapsed 10m 10s (remain 0m 44s) Loss: 0.1412(0.0995) Grad: 168399.0938  LR: 0.00001057  \nEpoch: [2][360/366] Elapsed 10m 44s (remain 0m 8s) Loss: 0.0860(0.0999) Grad: 91768.9531  LR: 0.00001014  \nEpoch: [2][365/366] Elapsed 10m 52s (remain 0m 0s) Loss: 0.1410(0.0999) Grad: 63975.6602  LR: 0.00001003  \nEVAL: [0/62] Elapsed 0m 1s (remain 1m 5s) Loss: 0.0976(0.0976) \nEVAL: [20/62] Elapsed 0m 25s (remain 0m 49s) Loss: 0.1000(0.1099) \nEVAL: [40/62] Elapsed 0m 49s (remain 0m 25s) Loss: 0.1012(0.1069) \nEVAL: [60/62] Elapsed 1m 16s (remain 0m 1s) Loss: 0.0908(0.1063) \nEVAL: [61/62] Elapsed 1m 16s (remain 0m 0s) Loss: 0.0675(0.1063) \n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 - avg_train_loss: 0.0999  avg_val_loss: 0.1063  time: 730s\nEpoch 2 - Score: 0.4617  Scores: [0.49752835357820074, 0.4550158366500907, 0.4202239361973426, 0.463478761311464, 0.47204326511794187, 0.46208142491791576]\nEpoch 2 - Save Best Score: 0.4617 Model\n","output_type":"stream"},{"name":"stdout","text":"Epoch: [3][0/366] Elapsed 0m 2s (remain 14m 21s) Loss: 0.0864(0.0864) Grad: 313597.5000  LR: 0.00001001  \nEpoch: [3][20/366] Elapsed 0m 34s (remain 9m 28s) Loss: 0.0851(0.0889) Grad: 172199.4375  LR: 0.00000958  \nEpoch: [3][40/366] Elapsed 1m 14s (remain 9m 48s) Loss: 0.0637(0.0905) Grad: 117987.2031  LR: 0.00000916  \nEpoch: [3][60/366] Elapsed 1m 44s (remain 8m 40s) Loss: 0.0719(0.0909) Grad: 73276.8516  LR: 0.00000873  \nEpoch: [3][80/366] Elapsed 2m 26s (remain 8m 36s) Loss: 0.1005(0.0915) Grad: 142133.6719  LR: 0.00000831  \nEpoch: [3][100/366] Elapsed 3m 0s (remain 7m 53s) Loss: 0.0794(0.0907) Grad: 74971.6562  LR: 0.00000789  \nEpoch: [3][120/366] Elapsed 3m 38s (remain 7m 22s) Loss: 0.0669(0.0915) Grad: 60914.0820  LR: 0.00000747  \nEpoch: [3][140/366] Elapsed 4m 14s (remain 6m 46s) Loss: 0.0685(0.0920) Grad: 77634.5156  LR: 0.00000706  \nEpoch: [3][160/366] Elapsed 4m 48s (remain 6m 7s) Loss: 0.0755(0.0925) Grad: 81616.4688  LR: 0.00000665  \nEpoch: [3][180/366] Elapsed 5m 26s (remain 5m 33s) Loss: 0.0675(0.0924) Grad: 52846.6914  LR: 0.00000625  \nEpoch: [3][200/366] Elapsed 5m 56s (remain 4m 53s) Loss: 0.0804(0.0924) Grad: 161479.4375  LR: 0.00000586  \nEpoch: [3][220/366] Elapsed 6m 29s (remain 4m 15s) Loss: 0.0959(0.0919) Grad: 109230.8203  LR: 0.00000547  \nEpoch: [3][240/366] Elapsed 7m 9s (remain 3m 42s) Loss: 0.1005(0.0919) Grad: 128255.7422  LR: 0.00000509  \nEpoch: [3][260/366] Elapsed 7m 43s (remain 3m 6s) Loss: 0.0575(0.0922) Grad: 62011.1211  LR: 0.00000472  \nEpoch: [3][280/366] Elapsed 8m 24s (remain 2m 32s) Loss: 0.0745(0.0914) Grad: 70426.3906  LR: 0.00000437  \nEpoch: [3][300/366] Elapsed 9m 3s (remain 1m 57s) Loss: 0.0812(0.0917) Grad: 45206.2266  LR: 0.00000402  \nEpoch: [3][320/366] Elapsed 9m 35s (remain 1m 20s) Loss: 0.1245(0.0913) Grad: 97909.8594  LR: 0.00000368  \nEpoch: [3][340/366] Elapsed 10m 11s (remain 0m 44s) Loss: 0.0892(0.0920) Grad: 81327.3438  LR: 0.00000335  \nEpoch: [3][360/366] Elapsed 10m 43s (remain 0m 8s) Loss: 0.0826(0.0924) Grad: 68593.2578  LR: 0.00000304  \nEpoch: [3][365/366] Elapsed 10m 54s (remain 0m 0s) Loss: 0.0944(0.0925) Grad: 69768.8750  LR: 0.00000296  \nEVAL: [0/62] Elapsed 0m 1s (remain 1m 7s) Loss: 0.1021(0.1021) \nEVAL: [20/62] Elapsed 0m 25s (remain 0m 49s) Loss: 0.0981(0.1084) \nEVAL: [40/62] Elapsed 0m 49s (remain 0m 25s) Loss: 0.0997(0.1065) \n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 - avg_train_loss: 0.0925  avg_val_loss: 0.1073  time: 732s\nEpoch 3 - Score: 0.4637  Scores: [0.5017682159513762, 0.452799945317173, 0.4208577474003596, 0.4658180232155834, 0.4847689411112601, 0.4562080463111953]\n","output_type":"stream"},{"name":"stdout","text":"EVAL: [60/62] Elapsed 1m 16s (remain 0m 1s) Loss: 0.0945(0.1073) \nEVAL: [61/62] Elapsed 1m 16s (remain 0m 0s) Loss: 0.0687(0.1073) \nEpoch: [4][0/366] Elapsed 0m 4s (remain 25m 55s) Loss: 0.0863(0.0863) Grad: 244026.2969  LR: 0.00000295  \nEpoch: [4][20/366] Elapsed 0m 37s (remain 10m 18s) Loss: 0.0885(0.0867) Grad: 206240.4062  LR: 0.00000265  \nEpoch: [4][40/366] Elapsed 1m 12s (remain 9m 32s) Loss: 0.1097(0.0918) Grad: 172582.5469  LR: 0.00000237  \nEpoch: [4][60/366] Elapsed 1m 45s (remain 8m 49s) Loss: 0.0922(0.0894) Grad: 148031.4688  LR: 0.00000210  \nEpoch: [4][80/366] Elapsed 2m 17s (remain 8m 2s) Loss: 0.0862(0.0898) Grad: 231147.6406  LR: 0.00000184  \nEpoch: [4][100/366] Elapsed 2m 49s (remain 7m 26s) Loss: 0.0627(0.0875) Grad: 83664.8359  LR: 0.00000160  \nEpoch: [4][120/366] Elapsed 3m 22s (remain 6m 50s) Loss: 0.1232(0.0882) Grad: 166292.9688  LR: 0.00000138  \nEpoch: [4][140/366] Elapsed 3m 55s (remain 6m 15s) Loss: 0.0648(0.0877) Grad: 156725.9375  LR: 0.00000117  \nEpoch: [4][160/366] Elapsed 4m 32s (remain 5m 47s) Loss: 0.1045(0.0877) Grad: 136180.2500  LR: 0.00000098  \nEpoch: [4][180/366] Elapsed 5m 7s (remain 5m 14s) Loss: 0.1033(0.0879) Grad: inf  LR: 0.00000080  \nEpoch: [4][200/366] Elapsed 5m 43s (remain 4m 41s) Loss: 0.0813(0.0887) Grad: 107834.6953  LR: 0.00000064  \nEpoch: [4][220/366] Elapsed 6m 20s (remain 4m 9s) Loss: 0.0749(0.0886) Grad: 105785.4922  LR: 0.00000050  \nEpoch: [4][240/366] Elapsed 6m 54s (remain 3m 34s) Loss: 0.0751(0.0883) Grad: 79238.3984  LR: 0.00000037  \nEpoch: [4][260/366] Elapsed 7m 34s (remain 3m 2s) Loss: 0.0752(0.0878) Grad: 75099.4219  LR: 0.00000027  \nEpoch: [4][280/366] Elapsed 8m 15s (remain 2m 29s) Loss: 0.0896(0.0878) Grad: 85867.2109  LR: 0.00000018  \nEpoch: [4][300/366] Elapsed 8m 47s (remain 1m 53s) Loss: 0.0610(0.0878) Grad: 41194.1367  LR: 0.00000011  \nEpoch: [4][320/366] Elapsed 9m 24s (remain 1m 19s) Loss: 0.1012(0.0871) Grad: 69925.6641  LR: 0.00000005  \nEpoch: [4][340/366] Elapsed 10m 5s (remain 0m 44s) Loss: 0.0721(0.0872) Grad: 79478.7500  LR: 0.00000002  \nEpoch: [4][360/366] Elapsed 10m 44s (remain 0m 8s) Loss: 0.0645(0.0869) Grad: 43793.8047  LR: 0.00000000  \nEpoch: [4][365/366] Elapsed 10m 54s (remain 0m 0s) Loss: 0.0526(0.0868) Grad: 60294.2031  LR: 0.00000000  \nEVAL: [0/62] Elapsed 0m 1s (remain 1m 4s) Loss: 0.0990(0.0990) \nEVAL: [20/62] Elapsed 0m 25s (remain 0m 49s) Loss: 0.1010(0.1072) \nEVAL: [40/62] Elapsed 0m 49s (remain 0m 25s) Loss: 0.0959(0.1041) \n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 - avg_train_loss: 0.0868  avg_val_loss: 0.1040  time: 731s\nEpoch 4 - Score: 0.4566  Scores: [0.49087000005766857, 0.4472768825903029, 0.41795184988624534, 0.4550918820034311, 0.4727581575642008, 0.45543938248561905]\nEpoch 4 - Save Best Score: 0.4566 Model\n","output_type":"stream"},{"name":"stdout","text":"EVAL: [60/62] Elapsed 1m 16s (remain 0m 1s) Loss: 0.0899(0.1040) \nEVAL: [61/62] Elapsed 1m 16s (remain 0m 0s) Loss: 0.0571(0.1040) \n","output_type":"stream"},{"name":"stderr","text":"========== fold: 1 result ==========\nScore: 0.4566  Scores: [0.49087000005766857, 0.4472768825903029, 0.41795184988624534, 0.4550918820034311, 0.4727581575642008, 0.45543938248561905]\n========== fold: 2 training ==========\nDebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_dropout\": 0.0,\n  \"attention_probs_dropout_prob\": 0.0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout\": 0.0,\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.21.2\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nSome weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias']\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Epoch: [1][0/366] Elapsed 0m 1s (remain 9m 39s) Loss: 2.5660(2.5660) Grad: inf  LR: 0.00002000  \nEpoch: [1][20/366] Elapsed 0m 35s (remain 9m 50s) Loss: 0.5007(1.5120) Grad: 159070.3438  LR: 0.00001999  \nEpoch: [1][40/366] Elapsed 1m 12s (remain 9m 38s) Loss: 0.2571(0.8962) Grad: 84917.3438  LR: 0.00001996  \nEpoch: [1][60/366] Elapsed 1m 52s (remain 9m 20s) Loss: 0.2290(0.6646) Grad: 145991.5156  LR: 0.00001991  \nEpoch: [1][80/366] Elapsed 2m 34s (remain 9m 4s) Loss: 0.1182(0.5533) Grad: 22527.8730  LR: 0.00001985  \nEpoch: [1][100/366] Elapsed 3m 12s (remain 8m 24s) Loss: 0.1254(0.4689) Grad: 19009.1621  LR: 0.00001977  \nEpoch: [1][120/366] Elapsed 3m 47s (remain 7m 40s) Loss: 0.1193(0.4143) Grad: 37480.4297  LR: 0.00001967  \nEpoch: [1][140/366] Elapsed 4m 21s (remain 6m 58s) Loss: 0.1004(0.3748) Grad: 26043.7461  LR: 0.00001955  \nEpoch: [1][160/366] Elapsed 5m 0s (remain 6m 22s) Loss: 0.1677(0.3446) Grad: 19904.7852  LR: 0.00001941  \nEpoch: [1][180/366] Elapsed 5m 36s (remain 5m 44s) Loss: 0.1056(0.3206) Grad: 50851.5352  LR: 0.00001926  \nEpoch: [1][200/366] Elapsed 6m 16s (remain 5m 8s) Loss: 0.0743(0.3002) Grad: 30911.1387  LR: 0.00001909  \nEpoch: [1][220/366] Elapsed 6m 48s (remain 4m 27s) Loss: 0.1026(0.2850) Grad: 33490.5938  LR: 0.00001890  \nEpoch: [1][240/366] Elapsed 7m 25s (remain 3m 50s) Loss: 0.1377(0.2715) Grad: 49103.8828  LR: 0.00001870  \nEpoch: [1][260/366] Elapsed 8m 4s (remain 3m 14s) Loss: 0.1596(0.2596) Grad: 31524.5254  LR: 0.00001848  \nEpoch: [1][280/366] Elapsed 8m 42s (remain 2m 37s) Loss: 0.1079(0.2504) Grad: 17405.2832  LR: 0.00001824  \nEpoch: [1][300/366] Elapsed 9m 18s (remain 2m 0s) Loss: 0.1367(0.2419) Grad: 48295.4219  LR: 0.00001799  \nEpoch: [1][320/366] Elapsed 9m 55s (remain 1m 23s) Loss: 0.1033(0.2339) Grad: 18895.4082  LR: 0.00001773  \nEpoch: [1][340/366] Elapsed 10m 28s (remain 0m 46s) Loss: 0.0866(0.2274) Grad: 19145.6699  LR: 0.00001745  \nEpoch: [1][360/366] Elapsed 11m 6s (remain 0m 9s) Loss: 0.1023(0.2213) Grad: 14308.2637  LR: 0.00001715  \nEpoch: [1][365/366] Elapsed 11m 14s (remain 0m 0s) Loss: 0.1063(0.2197) Grad: 17173.3086  LR: 0.00001708  \nEVAL: [0/62] Elapsed 0m 2s (remain 2m 26s) Loss: 0.1511(0.1511) \nEVAL: [20/62] Elapsed 0m 25s (remain 0m 48s) Loss: 0.0747(0.1221) \nEVAL: [40/62] Elapsed 0m 45s (remain 0m 23s) Loss: 0.1125(0.1211) \nEVAL: [60/62] Elapsed 1m 8s (remain 0m 1s) Loss: 0.0938(0.1216) \nEVAL: [61/62] Elapsed 1m 8s (remain 0m 0s) Loss: 0.0778(0.1215) \n","output_type":"stream"},{"name":"stderr","text":"Epoch 1 - avg_train_loss: 0.2197  avg_val_loss: 0.1215  time: 744s\nEpoch 1 - Score: 0.4950  Scores: [0.5066886811249021, 0.5262792771250718, 0.4911530397913427, 0.48639971576760155, 0.4787858575036432, 0.4809833578307154]\nEpoch 1 - Save Best Score: 0.4950 Model\n","output_type":"stream"},{"name":"stdout","text":"Epoch: [2][0/366] Elapsed 0m 2s (remain 12m 20s) Loss: 0.1199(0.1199) Grad: inf  LR: 0.00001706  \nEpoch: [2][20/366] Elapsed 0m 34s (remain 9m 23s) Loss: 0.1034(0.1167) Grad: 40987.9883  LR: 0.00001675  \nEpoch: [2][40/366] Elapsed 1m 12s (remain 9m 33s) Loss: 0.1095(0.1143) Grad: 41641.3633  LR: 0.00001643  \nEpoch: [2][60/366] Elapsed 1m 52s (remain 9m 20s) Loss: 0.0710(0.1073) Grad: 45657.8086  LR: 0.00001610  \nEpoch: [2][80/366] Elapsed 2m 27s (remain 8m 38s) Loss: 0.0906(0.1026) Grad: 49987.2461  LR: 0.00001575  \nEpoch: [2][100/366] Elapsed 3m 1s (remain 7m 56s) Loss: 0.1142(0.1022) Grad: 79379.6172  LR: 0.00001540  \nEpoch: [2][120/366] Elapsed 3m 36s (remain 7m 18s) Loss: 0.1081(0.1031) Grad: 57522.2383  LR: 0.00001503  \nEpoch: [2][140/366] Elapsed 4m 14s (remain 6m 45s) Loss: 0.1282(0.1039) Grad: 38508.8281  LR: 0.00001466  \nEpoch: [2][160/366] Elapsed 4m 57s (remain 6m 18s) Loss: 0.1207(0.1045) Grad: 80096.7031  LR: 0.00001427  \nEpoch: [2][180/366] Elapsed 5m 41s (remain 5m 48s) Loss: 0.1164(0.1042) Grad: 56153.9297  LR: 0.00001388  \nEpoch: [2][200/366] Elapsed 6m 15s (remain 5m 8s) Loss: 0.1524(0.1031) Grad: 82434.1953  LR: 0.00001348  \nEpoch: [2][220/366] Elapsed 6m 49s (remain 4m 28s) Loss: 0.1237(0.1034) Grad: 60497.7617  LR: 0.00001308  \nEpoch: [2][240/366] Elapsed 7m 27s (remain 3m 51s) Loss: 0.0937(0.1035) Grad: 35687.1875  LR: 0.00001267  \nEpoch: [2][260/366] Elapsed 8m 3s (remain 3m 14s) Loss: 0.1239(0.1038) Grad: 56016.3555  LR: 0.00001225  \nEpoch: [2][280/366] Elapsed 8m 42s (remain 2m 37s) Loss: 0.1108(0.1037) Grad: 75982.4219  LR: 0.00001183  \nEpoch: [2][300/366] Elapsed 9m 17s (remain 2m 0s) Loss: 0.1028(0.1035) Grad: 31126.0137  LR: 0.00001141  \nEpoch: [2][320/366] Elapsed 9m 50s (remain 1m 22s) Loss: 0.0942(0.1035) Grad: 49968.8281  LR: 0.00001098  \nEpoch: [2][340/366] Elapsed 10m 31s (remain 0m 46s) Loss: 0.0786(0.1031) Grad: 39035.0156  LR: 0.00001056  \nEpoch: [2][360/366] Elapsed 11m 8s (remain 0m 9s) Loss: 0.1009(0.1035) Grad: 40019.6250  LR: 0.00001013  \nEpoch: [2][365/366] Elapsed 11m 18s (remain 0m 0s) Loss: 0.0660(0.1033) Grad: 35502.8398  LR: 0.00001002  \nEVAL: [0/62] Elapsed 0m 2s (remain 2m 48s) Loss: 0.1376(0.1376) \nEVAL: [20/62] Elapsed 0m 25s (remain 0m 48s) Loss: 0.0955(0.1082) \nEVAL: [40/62] Elapsed 0m 45s (remain 0m 23s) Loss: 0.0990(0.1070) \nEVAL: [60/62] Elapsed 1m 8s (remain 0m 1s) Loss: 0.0829(0.1083) \nEVAL: [61/62] Elapsed 1m 8s (remain 0m 0s) Loss: 0.1086(0.1083) \n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 - avg_train_loss: 0.1033  avg_val_loss: 0.1083  time: 748s\nEpoch 2 - Score: 0.4662  Scores: [0.5068387293927298, 0.4526687429957325, 0.4197794108151496, 0.4748254114498776, 0.48377557374276126, 0.45908041772609054]\nEpoch 2 - Save Best Score: 0.4662 Model\n","output_type":"stream"},{"name":"stdout","text":"Epoch: [3][0/366] Elapsed 0m 2s (remain 12m 13s) Loss: 0.0751(0.0751) Grad: 155027.2031  LR: 0.00001000  \nEpoch: [3][20/366] Elapsed 0m 42s (remain 11m 44s) Loss: 0.1210(0.0865) Grad: 167815.2031  LR: 0.00000957  \nEpoch: [3][40/366] Elapsed 1m 16s (remain 10m 8s) Loss: 0.0762(0.0879) Grad: 65799.1406  LR: 0.00000914  \nEpoch: [3][60/366] Elapsed 1m 50s (remain 9m 11s) Loss: 0.0935(0.0875) Grad: 91353.3672  LR: 0.00000872  \nEpoch: [3][80/366] Elapsed 2m 23s (remain 8m 23s) Loss: 0.0829(0.0873) Grad: 86970.3906  LR: 0.00000829  \nEpoch: [3][100/366] Elapsed 3m 4s (remain 8m 4s) Loss: 0.1161(0.0874) Grad: 160104.0156  LR: 0.00000787  \nEpoch: [3][120/366] Elapsed 3m 41s (remain 7m 27s) Loss: 0.0549(0.0875) Grad: 51764.0273  LR: 0.00000746  \nEpoch: [3][140/366] Elapsed 4m 19s (remain 6m 54s) Loss: 0.0821(0.0876) Grad: 60280.7266  LR: 0.00000704  \nEpoch: [3][160/366] Elapsed 4m 52s (remain 6m 12s) Loss: 0.0820(0.0874) Grad: 49449.0859  LR: 0.00000664  \nEpoch: [3][180/366] Elapsed 5m 36s (remain 5m 44s) Loss: 0.0681(0.0877) Grad: 67956.0625  LR: 0.00000624  \nEpoch: [3][200/366] Elapsed 6m 9s (remain 5m 3s) Loss: 0.0941(0.0888) Grad: 70510.8125  LR: 0.00000584  \nEpoch: [3][220/366] Elapsed 6m 44s (remain 4m 25s) Loss: 0.0825(0.0890) Grad: 67566.1406  LR: 0.00000546  \nEpoch: [3][240/366] Elapsed 7m 23s (remain 3m 49s) Loss: 0.0958(0.0892) Grad: 69756.3828  LR: 0.00000508  \nEpoch: [3][260/366] Elapsed 8m 2s (remain 3m 14s) Loss: 0.0818(0.0894) Grad: 53750.6602  LR: 0.00000471  \nEpoch: [3][280/366] Elapsed 8m 37s (remain 2m 36s) Loss: 0.1144(0.0894) Grad: 86267.9531  LR: 0.00000435  \nEpoch: [3][300/366] Elapsed 9m 12s (remain 1m 59s) Loss: 0.0861(0.0898) Grad: 145905.5781  LR: 0.00000400  \nEpoch: [3][320/366] Elapsed 9m 54s (remain 1m 23s) Loss: 0.0680(0.0902) Grad: 78214.7109  LR: 0.00000367  \nEpoch: [3][340/366] Elapsed 10m 31s (remain 0m 46s) Loss: 0.0825(0.0906) Grad: 75392.5859  LR: 0.00000334  \nEpoch: [3][360/366] Elapsed 11m 11s (remain 0m 9s) Loss: 0.0670(0.0906) Grad: 108446.9531  LR: 0.00000303  \nEpoch: [3][365/366] Elapsed 11m 21s (remain 0m 0s) Loss: 0.1023(0.0906) Grad: 159424.2344  LR: 0.00000295  \nEVAL: [0/62] Elapsed 0m 2s (remain 2m 16s) Loss: 0.1363(0.1363) \nEVAL: [20/62] Elapsed 0m 24s (remain 0m 47s) Loss: 0.0852(0.1068) \nEVAL: [40/62] Elapsed 0m 45s (remain 0m 23s) Loss: 0.0944(0.1052) \nEVAL: [60/62] Elapsed 1m 7s (remain 0m 1s) Loss: 0.0857(0.1064) \nEVAL: [61/62] Elapsed 1m 7s (remain 0m 0s) Loss: 0.0908(0.1063) \n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 - avg_train_loss: 0.0906  avg_val_loss: 0.1063  time: 750s\nEpoch 3 - Score: 0.4621  Scores: [0.49386576976221075, 0.4507017189489425, 0.42018080227502924, 0.4673401076587974, 0.4832077438647709, 0.45707146213586847]\nEpoch 3 - Save Best Score: 0.4621 Model\n","output_type":"stream"},{"name":"stdout","text":"Epoch: [4][0/366] Elapsed 0m 2s (remain 12m 24s) Loss: 0.1177(0.1177) Grad: inf  LR: 0.00000294  \nEpoch: [4][20/366] Elapsed 0m 37s (remain 10m 21s) Loss: 0.1072(0.0892) Grad: 65952.2734  LR: 0.00000264  \nEpoch: [4][40/366] Elapsed 1m 12s (remain 9m 36s) Loss: 0.0530(0.0873) Grad: 76612.4531  LR: 0.00000236  \nEpoch: [4][60/366] Elapsed 1m 49s (remain 9m 5s) Loss: 0.0720(0.0887) Grad: 88741.2422  LR: 0.00000209  \nEpoch: [4][80/366] Elapsed 2m 34s (remain 9m 4s) Loss: 0.0857(0.0888) Grad: 71212.1875  LR: 0.00000183  \nEpoch: [4][100/366] Elapsed 3m 7s (remain 8m 12s) Loss: 0.0904(0.0880) Grad: 116529.2109  LR: 0.00000159  \nEpoch: [4][120/366] Elapsed 3m 43s (remain 7m 32s) Loss: 0.0924(0.0864) Grad: 76278.3594  LR: 0.00000137  \nEpoch: [4][140/366] Elapsed 4m 19s (remain 6m 54s) Loss: 0.0733(0.0862) Grad: 81473.2891  LR: 0.00000116  \nEpoch: [4][160/366] Elapsed 4m 54s (remain 6m 14s) Loss: 0.1139(0.0861) Grad: 75481.2812  LR: 0.00000097  \nEpoch: [4][180/366] Elapsed 5m 31s (remain 5m 39s) Loss: 0.0648(0.0859) Grad: 68858.5781  LR: 0.00000079  \nEpoch: [4][200/366] Elapsed 6m 6s (remain 5m 0s) Loss: 0.1171(0.0861) Grad: 132066.6875  LR: 0.00000063  \nEpoch: [4][220/366] Elapsed 6m 42s (remain 4m 23s) Loss: 0.0798(0.0866) Grad: 68960.2031  LR: 0.00000049  \nEpoch: [4][240/366] Elapsed 7m 18s (remain 3m 47s) Loss: 0.0912(0.0864) Grad: 49062.9336  LR: 0.00000037  \nEpoch: [4][260/366] Elapsed 7m 53s (remain 3m 10s) Loss: 0.0855(0.0864) Grad: 67126.9766  LR: 0.00000026  \nEpoch: [4][280/366] Elapsed 8m 34s (remain 2m 35s) Loss: 0.0792(0.0868) Grad: 82982.6094  LR: 0.00000017  \nEpoch: [4][300/366] Elapsed 9m 13s (remain 1m 59s) Loss: 0.1255(0.0865) Grad: 118067.3828  LR: 0.00000010  \nEpoch: [4][320/366] Elapsed 9m 51s (remain 1m 22s) Loss: 0.0776(0.0862) Grad: 111274.6172  LR: 0.00000005  \nEpoch: [4][340/366] Elapsed 10m 29s (remain 0m 46s) Loss: 0.0886(0.0862) Grad: 92795.4844  LR: 0.00000002  \nEpoch: [4][360/366] Elapsed 11m 12s (remain 0m 9s) Loss: 0.0759(0.0861) Grad: 83236.1250  LR: 0.00000000  \nEpoch: [4][365/366] Elapsed 11m 18s (remain 0m 0s) Loss: 0.1108(0.0861) Grad: 91257.1562  LR: 0.00000000  \nEVAL: [0/62] Elapsed 0m 2s (remain 2m 16s) Loss: 0.1352(0.1352) \nEVAL: [20/62] Elapsed 0m 24s (remain 0m 47s) Loss: 0.0815(0.1057) \nEVAL: [40/62] Elapsed 0m 45s (remain 0m 23s) Loss: 0.0904(0.1044) \nEVAL: [60/62] Elapsed 1m 7s (remain 0m 1s) Loss: 0.0887(0.1057) \nEVAL: [61/62] Elapsed 1m 8s (remain 0m 0s) Loss: 0.0815(0.1057) \n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 - avg_train_loss: 0.0861  avg_val_loss: 0.1057  time: 747s\nEpoch 4 - Score: 0.4609  Scores: [0.48631673808908726, 0.4520924097499547, 0.4199281527108704, 0.4674843281705815, 0.4824765906021695, 0.4570539684859164]\nEpoch 4 - Save Best Score: 0.4609 Model\n========== fold: 2 result ==========\nScore: 0.4609  Scores: [0.48631673808908726, 0.4520924097499547, 0.4199281527108704, 0.4674843281705815, 0.4824765906021695, 0.4570539684859164]\n========== fold: 3 training ==========\nDebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_dropout\": 0.0,\n  \"attention_probs_dropout_prob\": 0.0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout\": 0.0,\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.21.2\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nSome weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias']\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Epoch: [1][0/366] Elapsed 0m 1s (remain 11m 45s) Loss: 2.5190(2.5190) Grad: inf  LR: 0.00002000  \nEpoch: [1][20/366] Elapsed 0m 32s (remain 8m 57s) Loss: 0.5678(1.4109) Grad: 231647.3750  LR: 0.00001999  \nEpoch: [1][40/366] Elapsed 1m 7s (remain 8m 56s) Loss: 0.1533(0.8209) Grad: 60471.5391  LR: 0.00001996  \nEpoch: [1][60/366] Elapsed 1m 40s (remain 8m 23s) Loss: 0.1222(0.6028) Grad: 107471.9062  LR: 0.00001991  \nEpoch: [1][80/366] Elapsed 2m 22s (remain 8m 22s) Loss: 0.2034(0.4908) Grad: 160605.2500  LR: 0.00001985  \nEpoch: [1][100/366] Elapsed 3m 2s (remain 7m 58s) Loss: 0.1795(0.4200) Grad: 68345.4922  LR: 0.00001977  \nEpoch: [1][120/366] Elapsed 3m 41s (remain 7m 28s) Loss: 0.1026(0.3741) Grad: 73779.3672  LR: 0.00001967  \nEpoch: [1][140/366] Elapsed 4m 21s (remain 6m 57s) Loss: 0.0837(0.3395) Grad: 52114.3164  LR: 0.00001955  \nEpoch: [1][160/366] Elapsed 5m 4s (remain 6m 27s) Loss: 0.2281(0.3146) Grad: 94768.8984  LR: 0.00001941  \nEpoch: [1][180/366] Elapsed 5m 39s (remain 5m 47s) Loss: 0.0981(0.2931) Grad: 27783.3438  LR: 0.00001926  \nEpoch: [1][200/366] Elapsed 6m 13s (remain 5m 6s) Loss: 0.0690(0.2757) Grad: 31190.3594  LR: 0.00001909  \nEpoch: [1][220/366] Elapsed 6m 47s (remain 4m 27s) Loss: 0.1432(0.2625) Grad: 23473.9531  LR: 0.00001890  \nEpoch: [1][240/366] Elapsed 7m 26s (remain 3m 51s) Loss: 0.0832(0.2505) Grad: 49497.8555  LR: 0.00001870  \nEpoch: [1][260/366] Elapsed 8m 3s (remain 3m 14s) Loss: 0.1625(0.2420) Grad: 69867.0078  LR: 0.00001848  \nEpoch: [1][280/366] Elapsed 8m 41s (remain 2m 37s) Loss: 0.1217(0.2346) Grad: 61745.0898  LR: 0.00001824  \nEpoch: [1][300/366] Elapsed 9m 20s (remain 2m 1s) Loss: 0.1236(0.2270) Grad: 93752.4062  LR: 0.00001799  \nEpoch: [1][320/366] Elapsed 9m 58s (remain 1m 23s) Loss: 0.1062(0.2210) Grad: 65899.5234  LR: 0.00001773  \nEpoch: [1][340/366] Elapsed 10m 34s (remain 0m 46s) Loss: 0.0743(0.2141) Grad: 40192.5938  LR: 0.00001745  \nEpoch: [1][360/366] Elapsed 11m 11s (remain 0m 9s) Loss: 0.1049(0.2084) Grad: 81004.4844  LR: 0.00001715  \nEpoch: [1][365/366] Elapsed 11m 20s (remain 0m 0s) Loss: 0.1873(0.2072) Grad: 53775.6602  LR: 0.00001708  \nEVAL: [0/62] Elapsed 0m 2s (remain 2m 25s) Loss: 0.1229(0.1229) \nEVAL: [20/62] Elapsed 0m 24s (remain 0m 47s) Loss: 0.0789(0.1078) \nEVAL: [40/62] Elapsed 0m 47s (remain 0m 24s) Loss: 0.1095(0.1110) \nEVAL: [60/62] Elapsed 1m 9s (remain 0m 1s) Loss: 0.0908(0.1102) \nEVAL: [61/62] Elapsed 1m 9s (remain 0m 0s) Loss: 0.0556(0.1101) \n","output_type":"stream"},{"name":"stderr","text":"Epoch 1 - avg_train_loss: 0.2072  avg_val_loss: 0.1101  time: 750s\nEpoch 1 - Score: 0.4708  Scores: [0.5049618442807529, 0.45251737616096804, 0.46973962499369143, 0.4527339470708234, 0.48411809896314983, 0.4609117708204981]\nEpoch 1 - Save Best Score: 0.4708 Model\n","output_type":"stream"},{"name":"stdout","text":"Epoch: [2][0/366] Elapsed 0m 2s (remain 13m 50s) Loss: 0.1348(0.1348) Grad: 194884.9844  LR: 0.00001706  \nEpoch: [2][20/366] Elapsed 0m 39s (remain 10m 50s) Loss: 0.0705(0.1225) Grad: 85129.8281  LR: 0.00001675  \nEpoch: [2][40/366] Elapsed 1m 11s (remain 9m 25s) Loss: 0.1033(0.1141) Grad: 65128.1641  LR: 0.00001643  \nEpoch: [2][60/366] Elapsed 1m 51s (remain 9m 18s) Loss: 0.1013(0.1120) Grad: 52602.9336  LR: 0.00001610  \nEpoch: [2][80/366] Elapsed 2m 27s (remain 8m 38s) Loss: 0.1476(0.1080) Grad: 213674.1719  LR: 0.00001575  \nEpoch: [2][100/366] Elapsed 3m 3s (remain 8m 1s) Loss: 0.0918(0.1064) Grad: 81290.1953  LR: 0.00001540  \nEpoch: [2][120/366] Elapsed 3m 42s (remain 7m 30s) Loss: 0.1021(0.1061) Grad: 86304.4062  LR: 0.00001503  \nEpoch: [2][140/366] Elapsed 4m 21s (remain 6m 56s) Loss: 0.1150(0.1067) Grad: 84840.0391  LR: 0.00001466  \nEpoch: [2][160/366] Elapsed 4m 59s (remain 6m 21s) Loss: 0.1317(0.1067) Grad: 138024.4375  LR: 0.00001427  \nEpoch: [2][180/366] Elapsed 5m 39s (remain 5m 46s) Loss: 0.0992(0.1060) Grad: 68864.4609  LR: 0.00001388  \nEpoch: [2][200/366] Elapsed 6m 13s (remain 5m 6s) Loss: 0.0754(0.1063) Grad: 66619.3281  LR: 0.00001348  \nEpoch: [2][220/366] Elapsed 6m 45s (remain 4m 25s) Loss: 0.1242(0.1060) Grad: 181104.3906  LR: 0.00001308  \nEpoch: [2][240/366] Elapsed 7m 20s (remain 3m 48s) Loss: 0.0517(0.1055) Grad: 80749.5703  LR: 0.00001267  \nEpoch: [2][260/366] Elapsed 7m 53s (remain 3m 10s) Loss: 0.1174(0.1054) Grad: 231277.3125  LR: 0.00001225  \nEpoch: [2][280/366] Elapsed 8m 30s (remain 2m 34s) Loss: 0.0915(0.1057) Grad: 115242.1406  LR: 0.00001183  \nEpoch: [2][300/366] Elapsed 9m 7s (remain 1m 58s) Loss: 0.0793(0.1054) Grad: 107689.8594  LR: 0.00001141  \nEpoch: [2][320/366] Elapsed 9m 50s (remain 1m 22s) Loss: 0.0989(0.1050) Grad: 113195.6094  LR: 0.00001098  \nEpoch: [2][340/366] Elapsed 10m 30s (remain 0m 46s) Loss: 0.0894(0.1051) Grad: 101179.8359  LR: 0.00001056  \nEpoch: [2][360/366] Elapsed 11m 6s (remain 0m 9s) Loss: 0.1177(0.1049) Grad: 78120.7891  LR: 0.00001013  \nEpoch: [2][365/366] Elapsed 11m 15s (remain 0m 0s) Loss: 0.1373(0.1049) Grad: 153884.8906  LR: 0.00001002  \nEVAL: [0/62] Elapsed 0m 2s (remain 2m 12s) Loss: 0.1188(0.1188) \nEVAL: [20/62] Elapsed 0m 23s (remain 0m 46s) Loss: 0.0688(0.1017) \nEVAL: [40/62] Elapsed 0m 46s (remain 0m 24s) Loss: 0.1034(0.1038) \nEVAL: [60/62] Elapsed 1m 9s (remain 0m 1s) Loss: 0.0827(0.1032) \nEVAL: [61/62] Elapsed 1m 9s (remain 0m 0s) Loss: 0.0501(0.1031) \n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 - avg_train_loss: 0.1049  avg_val_loss: 0.1031  time: 745s\nEpoch 2 - Score: 0.4549  Scores: [0.49515794941773156, 0.44858525156303825, 0.41839494604962896, 0.44711434026874297, 0.47313860207662806, 0.4469806977988536]\nEpoch 2 - Save Best Score: 0.4549 Model\n","output_type":"stream"},{"name":"stdout","text":"Epoch: [3][0/366] Elapsed 0m 1s (remain 11m 34s) Loss: 0.1102(0.1102) Grad: 389988.0312  LR: 0.00001000  \nEpoch: [3][20/366] Elapsed 0m 43s (remain 11m 49s) Loss: 0.0725(0.1033) Grad: 201536.8594  LR: 0.00000957  \nEpoch: [3][40/366] Elapsed 1m 27s (remain 11m 33s) Loss: 0.0795(0.0993) Grad: 169988.9688  LR: 0.00000914  \nEpoch: [3][60/366] Elapsed 1m 58s (remain 9m 54s) Loss: 0.1225(0.0969) Grad: 116219.0234  LR: 0.00000872  \nEpoch: [3][80/366] Elapsed 2m 34s (remain 9m 3s) Loss: 0.1096(0.0973) Grad: 169317.4219  LR: 0.00000829  \nEpoch: [3][100/366] Elapsed 3m 14s (remain 8m 31s) Loss: 0.1396(0.0975) Grad: 151758.3438  LR: 0.00000787  \nEpoch: [3][120/366] Elapsed 3m 50s (remain 7m 46s) Loss: 0.0904(0.0957) Grad: 214036.3438  LR: 0.00000746  \nEpoch: [3][140/366] Elapsed 4m 39s (remain 7m 26s) Loss: 0.0598(0.0949) Grad: 170795.7188  LR: 0.00000704  \nEpoch: [3][160/366] Elapsed 5m 18s (remain 6m 45s) Loss: 0.1022(0.0952) Grad: 120867.3906  LR: 0.00000664  \nEpoch: [3][180/366] Elapsed 5m 55s (remain 6m 3s) Loss: 0.0707(0.0947) Grad: 95376.3359  LR: 0.00000624  \nEpoch: [3][200/366] Elapsed 6m 33s (remain 5m 22s) Loss: 0.0928(0.0950) Grad: 120524.9844  LR: 0.00000584  \nEpoch: [3][220/366] Elapsed 7m 13s (remain 4m 44s) Loss: 0.0964(0.0953) Grad: 103638.9922  LR: 0.00000546  \nEpoch: [3][240/366] Elapsed 7m 43s (remain 4m 0s) Loss: 0.0750(0.0946) Grad: 80496.8984  LR: 0.00000508  \nEpoch: [3][260/366] Elapsed 8m 18s (remain 3m 20s) Loss: 0.0570(0.0952) Grad: 117357.8281  LR: 0.00000471  \nEpoch: [3][280/366] Elapsed 8m 47s (remain 2m 39s) Loss: 0.0709(0.0954) Grad: 80596.1719  LR: 0.00000435  \nEpoch: [3][300/366] Elapsed 9m 20s (remain 2m 1s) Loss: 0.0835(0.0961) Grad: 129662.9609  LR: 0.00000400  \nEpoch: [3][320/366] Elapsed 9m 58s (remain 1m 23s) Loss: 0.1211(0.0957) Grad: 76444.2578  LR: 0.00000367  \nEpoch: [3][340/366] Elapsed 10m 32s (remain 0m 46s) Loss: 0.1206(0.0958) Grad: 122631.0234  LR: 0.00000334  \nEpoch: [3][360/366] Elapsed 11m 7s (remain 0m 9s) Loss: 0.0928(0.0959) Grad: 131825.5625  LR: 0.00000303  \nEpoch: [3][365/366] Elapsed 11m 20s (remain 0m 0s) Loss: 0.0862(0.0959) Grad: 58996.6680  LR: 0.00000295  \nEVAL: [0/62] Elapsed 0m 2s (remain 2m 12s) Loss: 0.0964(0.0964) \nEVAL: [20/62] Elapsed 0m 23s (remain 0m 46s) Loss: 0.0758(0.0990) \nEVAL: [40/62] Elapsed 0m 46s (remain 0m 24s) Loss: 0.1010(0.1006) \nEVAL: [60/62] Elapsed 1m 9s (remain 0m 1s) Loss: 0.0804(0.1008) \nEVAL: [61/62] Elapsed 1m 9s (remain 0m 0s) Loss: 0.0607(0.1008) \n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 - avg_train_loss: 0.0959  avg_val_loss: 0.1008  time: 750s\nEpoch 3 - Score: 0.4496  Scores: [0.48709626958399205, 0.44233973398771265, 0.41174531438016304, 0.442254147919475, 0.47415301213849786, 0.44007555368226237]\nEpoch 3 - Save Best Score: 0.4496 Model\n","output_type":"stream"},{"name":"stdout","text":"Epoch: [4][0/366] Elapsed 0m 1s (remain 12m 2s) Loss: 0.0919(0.0919) Grad: 196312.8125  LR: 0.00000294  \nEpoch: [4][20/366] Elapsed 0m 40s (remain 11m 4s) Loss: 0.1461(0.0957) Grad: 155370.3438  LR: 0.00000264  \nEpoch: [4][40/366] Elapsed 1m 15s (remain 9m 54s) Loss: 0.0611(0.0914) Grad: 108454.8594  LR: 0.00000236  \nEpoch: [4][60/366] Elapsed 1m 52s (remain 9m 22s) Loss: 0.0671(0.0905) Grad: 130470.2031  LR: 0.00000209  \nEpoch: [4][80/366] Elapsed 2m 29s (remain 8m 46s) Loss: 0.0885(0.0908) Grad: 110991.9375  LR: 0.00000183  \nEpoch: [4][100/366] Elapsed 3m 2s (remain 7m 58s) Loss: 0.0790(0.0905) Grad: 237135.7031  LR: 0.00000159  \nEpoch: [4][120/366] Elapsed 3m 37s (remain 7m 20s) Loss: 0.0971(0.0909) Grad: 183717.5781  LR: 0.00000137  \nEpoch: [4][140/366] Elapsed 4m 10s (remain 6m 39s) Loss: 0.0926(0.0914) Grad: 129512.0312  LR: 0.00000116  \nEpoch: [4][160/366] Elapsed 4m 50s (remain 6m 10s) Loss: 0.1295(0.0920) Grad: 203202.3750  LR: 0.00000097  \nEpoch: [4][180/366] Elapsed 5m 32s (remain 5m 39s) Loss: 0.1134(0.0916) Grad: 197283.4375  LR: 0.00000079  \nEpoch: [4][200/366] Elapsed 6m 5s (remain 4m 59s) Loss: 0.0522(0.0913) Grad: 119329.3672  LR: 0.00000063  \nEpoch: [4][220/366] Elapsed 6m 42s (remain 4m 24s) Loss: 0.0737(0.0907) Grad: 140731.9688  LR: 0.00000049  \nEpoch: [4][240/366] Elapsed 7m 19s (remain 3m 47s) Loss: 0.0845(0.0905) Grad: 220378.6875  LR: 0.00000037  \nEpoch: [4][260/366] Elapsed 7m 52s (remain 3m 9s) Loss: 0.0752(0.0907) Grad: 154688.3281  LR: 0.00000026  \nEpoch: [4][280/366] Elapsed 8m 36s (remain 2m 36s) Loss: 0.1008(0.0905) Grad: 300091.6562  LR: 0.00000017  \nEpoch: [4][300/366] Elapsed 9m 14s (remain 1m 59s) Loss: 0.0718(0.0904) Grad: 123373.6406  LR: 0.00000010  \nEpoch: [4][320/366] Elapsed 9m 44s (remain 1m 21s) Loss: 0.0850(0.0903) Grad: 218548.2812  LR: 0.00000005  \nEpoch: [4][340/366] Elapsed 10m 22s (remain 0m 45s) Loss: 0.0766(0.0900) Grad: 135784.6719  LR: 0.00000002  \nEpoch: [4][360/366] Elapsed 10m 59s (remain 0m 9s) Loss: 0.1025(0.0903) Grad: 155981.5000  LR: 0.00000000  \nEpoch: [4][365/366] Elapsed 11m 11s (remain 0m 0s) Loss: 0.1021(0.0905) Grad: 160669.7188  LR: 0.00000000  \nEVAL: [0/62] Elapsed 0m 2s (remain 2m 12s) Loss: 0.0990(0.0990) \nEVAL: [20/62] Elapsed 0m 23s (remain 0m 46s) Loss: 0.0728(0.0981) \nEVAL: [40/62] Elapsed 0m 46s (remain 0m 24s) Loss: 0.1006(0.0996) \nEVAL: [60/62] Elapsed 1m 9s (remain 0m 1s) Loss: 0.0795(0.0998) \nEVAL: [61/62] Elapsed 1m 9s (remain 0m 0s) Loss: 0.0577(0.0997) \n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 - avg_train_loss: 0.0905  avg_val_loss: 0.0997  time: 741s\nEpoch 4 - Score: 0.4472  Scores: [0.48615718866071755, 0.44180628948712086, 0.41085664588050547, 0.4390828868670061, 0.4663456380553179, 0.43908704802755355]\nEpoch 4 - Save Best Score: 0.4472 Model\n========== fold: 3 result ==========\nScore: 0.4472  Scores: [0.48615718866071755, 0.44180628948712086, 0.41085664588050547, 0.4390828868670061, 0.4663456380553179, 0.43908704802755355]\n========== CV ==========\nScore: 0.4541  Scores: [0.4882378087810526, 0.44598236938101576, 0.4146667870360092, 0.4551777290731584, 0.47200463415827215, 0.448399797576739]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>█▂▁▁</td></tr><tr><td>[fold0] avg_val_loss</td><td>█▅▃▁</td></tr><tr><td>[fold0] epoch</td><td>▁▃▆█</td></tr><tr><td>[fold0] loss</td><td>█▂▂▂▂▁▁▂▂▁▁▁▂▁▁▁▁▁▂▁▂▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▂</td></tr><tr><td>[fold0] lr</td><td>███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold0] score</td><td>█▅▃▁</td></tr><tr><td>[fold1] avg_train_loss</td><td>█▂▁▁</td></tr><tr><td>[fold1] avg_val_loss</td><td>█▂▂▁</td></tr><tr><td>[fold1] epoch</td><td>▁▃▆█</td></tr><tr><td>[fold1] loss</td><td>█▂▂▂▁▂▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁</td></tr><tr><td>[fold1] lr</td><td>███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold1] score</td><td>█▂▂▁</td></tr><tr><td>[fold2] avg_train_loss</td><td>█▂▁▁</td></tr><tr><td>[fold2] avg_val_loss</td><td>█▂▁▁</td></tr><tr><td>[fold2] epoch</td><td>▁▃▆█</td></tr><tr><td>[fold2] loss</td><td>█▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>[fold2] lr</td><td>███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold2] score</td><td>█▂▁▁</td></tr><tr><td>[fold3] avg_train_loss</td><td>█▂▁▁</td></tr><tr><td>[fold3] avg_val_loss</td><td>█▃▂▁</td></tr><tr><td>[fold3] epoch</td><td>▁▃▆█</td></tr><tr><td>[fold3] loss</td><td>█▂▂▂▂▁▂▁▂▁▂▁▂▁▁▂▁▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▂</td></tr><tr><td>[fold3] lr</td><td>███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold3] score</td><td>█▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>0.08829</td></tr><tr><td>[fold0] avg_val_loss</td><td>0.10179</td></tr><tr><td>[fold0] epoch</td><td>4</td></tr><tr><td>[fold0] loss</td><td>0.08707</td></tr><tr><td>[fold0] lr</td><td>0.0</td></tr><tr><td>[fold0] score</td><td>0.45145</td></tr><tr><td>[fold1] avg_train_loss</td><td>0.08681</td></tr><tr><td>[fold1] avg_val_loss</td><td>0.104</td></tr><tr><td>[fold1] epoch</td><td>4</td></tr><tr><td>[fold1] loss</td><td>0.05258</td></tr><tr><td>[fold1] lr</td><td>0.0</td></tr><tr><td>[fold1] score</td><td>0.45656</td></tr><tr><td>[fold2] avg_train_loss</td><td>0.0861</td></tr><tr><td>[fold2] avg_val_loss</td><td>0.1057</td></tr><tr><td>[fold2] epoch</td><td>4</td></tr><tr><td>[fold2] loss</td><td>0.11076</td></tr><tr><td>[fold2] lr</td><td>0.0</td></tr><tr><td>[fold2] score</td><td>0.46089</td></tr><tr><td>[fold3] avg_train_loss</td><td>0.09046</td></tr><tr><td>[fold3] avg_val_loss</td><td>0.0997</td></tr><tr><td>[fold3] epoch</td><td>4</td></tr><tr><td>[fold3] loss</td><td>0.10209</td></tr><tr><td>[fold3] lr</td><td>0.0</td></tr><tr><td>[fold3] score</td><td>0.44722</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Synced <strong style=\"color:#cdcd00\">microsoft/deberta-v3-base</strong>: <a href=\"https://wandb.ai/minori11/FB3-Public/runs/36gbep3p\" target=\"_blank\">https://wandb.ai/minori11/FB3-Public/runs/36gbep3p</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20221030_033031-36gbep3p/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}